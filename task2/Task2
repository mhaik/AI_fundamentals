
import streamlit as st
import numpy as np
from matplotlib import pyplot as plt
import math

st.title("Single neuron, Task 2")

# task 1 basis
def generator(num_modes, num_samples, mean_range, cov_range):
    samples = []
    for mode in range(num_modes):
        mean_x, mean_y = np.random.uniform(mean_range[0], mean_range[1], 2)
        sigma_x, sigma_y = np.random.uniform(cov_range[0], cov_range[1], 2)
        x_samples = np.random.normal(mean_x, sigma_x, num_samples)
        y_samples = np.random.normal(mean_y, sigma_y, num_samples)
        mode_samples = np.column_stack((x_samples, y_samples))
        samples.append(mode_samples)
    samples = np.vstack(samples)
    return samples

class Neuron:
    def __init__(self, input_dim, activation='heaviside'):
        self.weights = np.random.randn(input_dim)  # random weights
        self.bias = np.random.randn()  # bias
        self.activation = activation # activiation function
        self.lr = 0.01  # learning rate

    # activation function
    def activate(self, s):
        if self.activation == 'heaviside':
            return 1 if s >= 0 else 0
        elif self.activation == 'logistic':
            return 1 / (1 + math.exp(-s))
        elif self.activation == 'tanh':
            return math.tanh(s)
        elif self.activation == 'sin':
            return math.sin(s)
        elif self.activation == 'sign':
            return 1 if s > 0 else -1 if s < 0 else 0
        elif self.activation == 'relu':
            return max(0, s)
        elif self.activation == 'leaky_relu':
            return s if s > 0 else 0.01 * s

    # derivative
    def activate_derivative(self, s):
        if self.activation == 'heaviside':
            return 1
        elif self.activation == 'logistic':  # sigmoid
            sig = self.activate(s)
            return sig * (1 - sig)
        return 1  # others (simplified)

    # prediction
    def predict(self, x):
        s = np.dot(self.weights, x) + self.bias
        return self.activate(s)

    # training
    def train(self, x, d):

        if self.activation == 'sin':
            d = 2 * d - 1  # [-1, 1]

        s = np.dot(self.weights, x) + self.bias
        y = self.activate(s)
        error = d - y
        weight_update = self.lr * error * self.activate_derivative(s) * x
        bias_update = self.lr * error * self.activate_derivative(s)
        self.weights += weight_update
        self.bias += bias_update

st.sidebar.header("Input")
num_samples = st.sidebar.slider("Number of samples", 100, 1000, 200)
num_modes = st.sidebar.slider("Number of modes", 1, 9, 1)
activation_function = st.sidebar.selectbox("Activation function", ['heaviside', 'logistic', 'tanh', 'sin', 'sign', 'relu', 'leaky_relu'])
epochs = st.sidebar.slider("Epochs", 1, 100, 10)

mean_range = [-1.0, 1.0]
cov_range = [0.01, 0.1]
samples_class_0 = generator(num_modes, num_samples // 2, mean_range, cov_range)
samples_class_1 = generator(num_modes, num_samples // 2, mean_range, cov_range)

neuron = Neuron(input_dim=2, activation=activation_function)

# train
for epoch in range(epochs):

    for sample in samples_class_0:
        target_label = 0
        neuron.train(sample, target_label)
    
    for sample in samples_class_1:
        target_label = 1
        neuron.train(sample, target_label)

# decision boundary
x_vals = np.arange(-1, 1, 0.01)
y_vals = np.arange(-1, 1, 0.01)
XX, YY = np.meshgrid(x_vals, y_vals)
Z = np.zeros_like(XX)  # Z same shape as XX and YY
for i in range(XX.shape[0]):
    for j in range(XX.shape[1]):
        point = np.array([XX[i, j], YY[i, j]])
        Z[i, j] = neuron.predict(point)

fig, ax = plt.subplots()
ax.contourf(XX, YY, Z, levels=[0, 0.5, 1], alpha=0.3, cmap="RdBu")
ax.scatter(samples_class_0[:, 0], samples_class_0[:, 1], color='red')
ax.scatter(samples_class_1[:, 0], samples_class_1[:, 1], color='blue')
ax.set_xlabel('X')
ax.set_ylabel('Y')
st.pyplot(fig)
